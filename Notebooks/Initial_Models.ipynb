{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PowerTransformer, FunctionTransformer, QuantileTransformer\n",
    "\n",
    "sys.path.append('../Scripts')\n",
    "from Data_Processing import DataProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataProcessing('../Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Lap_Time']\n",
    "X = df.drop(columns=['Lap_Time'])\n",
    "\n",
    "obj_columns = X.select_dtypes(include=object).columns\n",
    "num_columns = X.select_dtypes(include='number').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcand\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:205: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "C:\\Users\\mcand\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:216: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lap_Number</th>\n",
       "      <th>Lap_Improvement</th>\n",
       "      <th>S1</th>\n",
       "      <th>S1_Improvement</th>\n",
       "      <th>S2</th>\n",
       "      <th>S2_Improvement</th>\n",
       "      <th>S3</th>\n",
       "      <th>S3_Improvement</th>\n",
       "      <th>Kph</th>\n",
       "      <th>Elapsed</th>\n",
       "      <th>Driver_Name</th>\n",
       "      <th>Pit_Time</th>\n",
       "      <th>Team</th>\n",
       "      <th>Power</th>\n",
       "      <th>Location</th>\n",
       "      <th>Event</th>\n",
       "      <th>Air_Temp</th>\n",
       "      <th>Track_Temp</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Wind_Speed</th>\n",
       "      <th>Wind_Direction</th>\n",
       "      <th>Rain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.746716</td>\n",
       "      <td>-0.280449</td>\n",
       "      <td>2.106329</td>\n",
       "      <td>-0.279033</td>\n",
       "      <td>0.552565</td>\n",
       "      <td>-0.280853</td>\n",
       "      <td>0.536325</td>\n",
       "      <td>-0.281457</td>\n",
       "      <td>-1.948791</td>\n",
       "      <td>-0.895229</td>\n",
       "      <td>SB</td>\n",
       "      <td>2.217140</td>\n",
       "      <td>JR</td>\n",
       "      <td>0.312523</td>\n",
       "      <td>Location 2</td>\n",
       "      <td>Free Practice 2</td>\n",
       "      <td>-0.983053</td>\n",
       "      <td>-0.927170</td>\n",
       "      <td>-0.640324</td>\n",
       "      <td>-1.003136</td>\n",
       "      <td>-0.656589</td>\n",
       "      <td>-0.059116</td>\n",
       "      <td>-0.604944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.405532</td>\n",
       "      <td>3.565709</td>\n",
       "      <td>-0.609054</td>\n",
       "      <td>3.583806</td>\n",
       "      <td>0.329821</td>\n",
       "      <td>3.560584</td>\n",
       "      <td>0.477411</td>\n",
       "      <td>3.552934</td>\n",
       "      <td>0.754053</td>\n",
       "      <td>-0.688184</td>\n",
       "      <td>SB</td>\n",
       "      <td>2.217140</td>\n",
       "      <td>JR</td>\n",
       "      <td>0.312523</td>\n",
       "      <td>Location 2</td>\n",
       "      <td>Free Practice 2</td>\n",
       "      <td>-0.983053</td>\n",
       "      <td>-0.923319</td>\n",
       "      <td>-0.640324</td>\n",
       "      <td>-1.003136</td>\n",
       "      <td>-0.579502</td>\n",
       "      <td>-0.187380</td>\n",
       "      <td>-0.604944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.122816</td>\n",
       "      <td>-0.280449</td>\n",
       "      <td>-0.154951</td>\n",
       "      <td>-0.279033</td>\n",
       "      <td>0.406868</td>\n",
       "      <td>-0.280853</td>\n",
       "      <td>1.028121</td>\n",
       "      <td>-0.281457</td>\n",
       "      <td>-0.152838</td>\n",
       "      <td>-0.474360</td>\n",
       "      <td>SB</td>\n",
       "      <td>2.217140</td>\n",
       "      <td>JR</td>\n",
       "      <td>0.312523</td>\n",
       "      <td>Location 2</td>\n",
       "      <td>Free Practice 2</td>\n",
       "      <td>-0.983053</td>\n",
       "      <td>-0.923319</td>\n",
       "      <td>-0.640324</td>\n",
       "      <td>-1.003149</td>\n",
       "      <td>-0.656589</td>\n",
       "      <td>-0.308750</td>\n",
       "      <td>-0.604944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.746716</td>\n",
       "      <td>-0.280449</td>\n",
       "      <td>1.700595</td>\n",
       "      <td>-0.279033</td>\n",
       "      <td>0.865064</td>\n",
       "      <td>-0.280853</td>\n",
       "      <td>1.015558</td>\n",
       "      <td>-0.281457</td>\n",
       "      <td>-1.093970</td>\n",
       "      <td>-1.657123</td>\n",
       "      <td>LGRA</td>\n",
       "      <td>-1.885707</td>\n",
       "      <td>AD</td>\n",
       "      <td>0.312523</td>\n",
       "      <td>Location 2</td>\n",
       "      <td>Free Practice 2</td>\n",
       "      <td>-0.983053</td>\n",
       "      <td>-0.931040</td>\n",
       "      <td>-0.640324</td>\n",
       "      <td>-1.003193</td>\n",
       "      <td>-0.759439</td>\n",
       "      <td>-0.224481</td>\n",
       "      <td>-0.604944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.405532</td>\n",
       "      <td>-0.280449</td>\n",
       "      <td>-0.173793</td>\n",
       "      <td>-0.279033</td>\n",
       "      <td>0.692353</td>\n",
       "      <td>-0.280853</td>\n",
       "      <td>0.593586</td>\n",
       "      <td>-0.281457</td>\n",
       "      <td>0.275269</td>\n",
       "      <td>-1.279886</td>\n",
       "      <td>LGRA</td>\n",
       "      <td>-1.885707</td>\n",
       "      <td>AD</td>\n",
       "      <td>0.312523</td>\n",
       "      <td>Location 2</td>\n",
       "      <td>Free Practice 2</td>\n",
       "      <td>-0.981675</td>\n",
       "      <td>-0.931040</td>\n",
       "      <td>-0.640324</td>\n",
       "      <td>-1.003180</td>\n",
       "      <td>-0.656589</td>\n",
       "      <td>-0.299331</td>\n",
       "      <td>-0.604944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10271</th>\n",
       "      <td>1.129661</td>\n",
       "      <td>-0.280449</td>\n",
       "      <td>-1.293815</td>\n",
       "      <td>-0.279033</td>\n",
       "      <td>-1.256324</td>\n",
       "      <td>-0.280853</td>\n",
       "      <td>-0.597339</td>\n",
       "      <td>-0.281457</td>\n",
       "      <td>0.998648</td>\n",
       "      <td>0.525703</td>\n",
       "      <td>PWEHRL</td>\n",
       "      <td>-1.380116</td>\n",
       "      <td>TAG</td>\n",
       "      <td>0.312523</td>\n",
       "      <td>Location 8</td>\n",
       "      <td>Free Practice 1</td>\n",
       "      <td>0.610959</td>\n",
       "      <td>-0.448898</td>\n",
       "      <td>1.538364</td>\n",
       "      <td>0.808092</td>\n",
       "      <td>0.065542</td>\n",
       "      <td>-0.095519</td>\n",
       "      <td>1.652951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10272</th>\n",
       "      <td>1.242452</td>\n",
       "      <td>-0.280449</td>\n",
       "      <td>-1.103377</td>\n",
       "      <td>-0.279033</td>\n",
       "      <td>-1.075166</td>\n",
       "      <td>-0.280853</td>\n",
       "      <td>-0.125200</td>\n",
       "      <td>-0.281457</td>\n",
       "      <td>0.270398</td>\n",
       "      <td>0.600429</td>\n",
       "      <td>PWEHRL</td>\n",
       "      <td>-1.380116</td>\n",
       "      <td>TAG</td>\n",
       "      <td>-3.199767</td>\n",
       "      <td>Location 8</td>\n",
       "      <td>Free Practice 1</td>\n",
       "      <td>0.611950</td>\n",
       "      <td>1.227820</td>\n",
       "      <td>1.537859</td>\n",
       "      <td>0.808106</td>\n",
       "      <td>-1.195045</td>\n",
       "      <td>0.181954</td>\n",
       "      <td>1.652951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10273</th>\n",
       "      <td>1.352021</td>\n",
       "      <td>-0.280449</td>\n",
       "      <td>2.028931</td>\n",
       "      <td>-0.279033</td>\n",
       "      <td>-1.038451</td>\n",
       "      <td>-0.280853</td>\n",
       "      <td>-0.547899</td>\n",
       "      <td>-0.281457</td>\n",
       "      <td>-1.749245</td>\n",
       "      <td>0.771174</td>\n",
       "      <td>PWEHRL</td>\n",
       "      <td>-1.380116</td>\n",
       "      <td>TAG</td>\n",
       "      <td>0.312523</td>\n",
       "      <td>Location 8</td>\n",
       "      <td>Free Practice 1</td>\n",
       "      <td>0.613389</td>\n",
       "      <td>1.229810</td>\n",
       "      <td>0.875448</td>\n",
       "      <td>0.808126</td>\n",
       "      <td>0.798311</td>\n",
       "      <td>0.190737</td>\n",
       "      <td>1.652951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10274</th>\n",
       "      <td>1.458615</td>\n",
       "      <td>3.565709</td>\n",
       "      <td>-1.413899</td>\n",
       "      <td>3.583806</td>\n",
       "      <td>-1.283153</td>\n",
       "      <td>3.560584</td>\n",
       "      <td>-0.610027</td>\n",
       "      <td>3.552934</td>\n",
       "      <td>1.078944</td>\n",
       "      <td>0.830491</td>\n",
       "      <td>PWEHRL</td>\n",
       "      <td>-1.380116</td>\n",
       "      <td>TAG</td>\n",
       "      <td>0.312523</td>\n",
       "      <td>Location 8</td>\n",
       "      <td>Free Practice 1</td>\n",
       "      <td>0.611722</td>\n",
       "      <td>1.229810</td>\n",
       "      <td>1.538829</td>\n",
       "      <td>0.808116</td>\n",
       "      <td>0.470116</td>\n",
       "      <td>0.784176</td>\n",
       "      <td>1.652951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10275</th>\n",
       "      <td>1.562451</td>\n",
       "      <td>-0.280449</td>\n",
       "      <td>-1.219278</td>\n",
       "      <td>-0.279033</td>\n",
       "      <td>-1.135640</td>\n",
       "      <td>-0.280853</td>\n",
       "      <td>-0.568254</td>\n",
       "      <td>-0.281457</td>\n",
       "      <td>0.858234</td>\n",
       "      <td>0.890061</td>\n",
       "      <td>PWEHRL</td>\n",
       "      <td>-1.380116</td>\n",
       "      <td>TAG</td>\n",
       "      <td>0.312523</td>\n",
       "      <td>Location 8</td>\n",
       "      <td>Free Practice 1</td>\n",
       "      <td>0.611035</td>\n",
       "      <td>1.227820</td>\n",
       "      <td>1.538558</td>\n",
       "      <td>0.808085</td>\n",
       "      <td>0.585072</td>\n",
       "      <td>0.102454</td>\n",
       "      <td>1.652951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10272 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Lap_Number  Lap_Improvement        S1  S1_Improvement        S2  \\\n",
       "0       -1.746716        -0.280449  2.106329       -0.279033  0.552565   \n",
       "1       -1.405532         3.565709 -0.609054        3.583806  0.329821   \n",
       "2       -1.122816        -0.280449 -0.154951       -0.279033  0.406868   \n",
       "3       -1.746716        -0.280449  1.700595       -0.279033  0.865064   \n",
       "4       -1.405532        -0.280449 -0.173793       -0.279033  0.692353   \n",
       "...           ...              ...       ...             ...       ...   \n",
       "10271    1.129661        -0.280449 -1.293815       -0.279033 -1.256324   \n",
       "10272    1.242452        -0.280449 -1.103377       -0.279033 -1.075166   \n",
       "10273    1.352021        -0.280449  2.028931       -0.279033 -1.038451   \n",
       "10274    1.458615         3.565709 -1.413899        3.583806 -1.283153   \n",
       "10275    1.562451        -0.280449 -1.219278       -0.279033 -1.135640   \n",
       "\n",
       "       S2_Improvement        S3  S3_Improvement       Kph   Elapsed  \\\n",
       "0           -0.280853  0.536325       -0.281457 -1.948791 -0.895229   \n",
       "1            3.560584  0.477411        3.552934  0.754053 -0.688184   \n",
       "2           -0.280853  1.028121       -0.281457 -0.152838 -0.474360   \n",
       "3           -0.280853  1.015558       -0.281457 -1.093970 -1.657123   \n",
       "4           -0.280853  0.593586       -0.281457  0.275269 -1.279886   \n",
       "...               ...       ...             ...       ...       ...   \n",
       "10271       -0.280853 -0.597339       -0.281457  0.998648  0.525703   \n",
       "10272       -0.280853 -0.125200       -0.281457  0.270398  0.600429   \n",
       "10273       -0.280853 -0.547899       -0.281457 -1.749245  0.771174   \n",
       "10274        3.560584 -0.610027        3.552934  1.078944  0.830491   \n",
       "10275       -0.280853 -0.568254       -0.281457  0.858234  0.890061   \n",
       "\n",
       "      Driver_Name  Pit_Time  Team     Power    Location            Event  \\\n",
       "0              SB  2.217140    JR  0.312523  Location 2  Free Practice 2   \n",
       "1              SB  2.217140    JR  0.312523  Location 2  Free Practice 2   \n",
       "2              SB  2.217140    JR  0.312523  Location 2  Free Practice 2   \n",
       "3            LGRA -1.885707    AD  0.312523  Location 2  Free Practice 2   \n",
       "4            LGRA -1.885707    AD  0.312523  Location 2  Free Practice 2   \n",
       "...           ...       ...   ...       ...         ...              ...   \n",
       "10271      PWEHRL -1.380116  TAG   0.312523  Location 8  Free Practice 1   \n",
       "10272      PWEHRL -1.380116  TAG  -3.199767  Location 8  Free Practice 1   \n",
       "10273      PWEHRL -1.380116  TAG   0.312523  Location 8  Free Practice 1   \n",
       "10274      PWEHRL -1.380116  TAG   0.312523  Location 8  Free Practice 1   \n",
       "10275      PWEHRL -1.380116  TAG   0.312523  Location 8  Free Practice 1   \n",
       "\n",
       "       Air_Temp  Track_Temp  Humidity  Pressure  Wind_Speed  Wind_Direction  \\\n",
       "0     -0.983053   -0.927170 -0.640324 -1.003136   -0.656589       -0.059116   \n",
       "1     -0.983053   -0.923319 -0.640324 -1.003136   -0.579502       -0.187380   \n",
       "2     -0.983053   -0.923319 -0.640324 -1.003149   -0.656589       -0.308750   \n",
       "3     -0.983053   -0.931040 -0.640324 -1.003193   -0.759439       -0.224481   \n",
       "4     -0.981675   -0.931040 -0.640324 -1.003180   -0.656589       -0.299331   \n",
       "...         ...         ...       ...       ...         ...             ...   \n",
       "10271  0.610959   -0.448898  1.538364  0.808092    0.065542       -0.095519   \n",
       "10272  0.611950    1.227820  1.537859  0.808106   -1.195045        0.181954   \n",
       "10273  0.613389    1.229810  0.875448  0.808126    0.798311        0.190737   \n",
       "10274  0.611722    1.229810  1.538829  0.808116    0.470116        0.784176   \n",
       "10275  0.611035    1.227820  1.538558  0.808085    0.585072        0.102454   \n",
       "\n",
       "           Rain  \n",
       "0     -0.604944  \n",
       "1     -0.604944  \n",
       "2     -0.604944  \n",
       "3     -0.604944  \n",
       "4     -0.604944  \n",
       "...         ...  \n",
       "10271  1.652951  \n",
       "10272  1.652951  \n",
       "10273  1.652951  \n",
       "10274  1.652951  \n",
       "10275  1.652951  \n",
       "\n",
       "[10272 rows x 23 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "X[num_columns] = pt.fit_transform(X[num_columns])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer = ColumnTransformer(\n",
    "[('num', StandardScaler(), num_columns),\n",
    "('obj', OneHotEncoder(), obj_columns)],\n",
    "remainder='passthrough')\n",
    "\n",
    "trans_X = column_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Models/Power_Transformer.pkl']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(column_transformer, '../Models/Column_Transformer.pkl')\n",
    "joblib.dump(pt, '../Models/Power_Transformer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trans_X = trans_X.toarray()\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'StandardScaler' has no attribute 'get_feature_names_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-9326078e34b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'StandardScaler' has no attribute 'get_feature_names_out'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(trans_X, y, random_state=42, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(K.log(1+y_pred) - K.log(1+y_true))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7190, 81)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = ModelCheckpoint(f'../Models/NN_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100, activation='relu', input_dim=81),\n",
    "    keras.layers.LeakyReLU(500),\n",
    "    keras.layers.LeakyReLU(800),\n",
    "    keras.layers.LeakyReLU(200),\n",
    "    keras.layers.LeakyReLU(200),\n",
    "    keras.layers.Dense(1, activation='relu')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=root_mean_squared_log_error,\n",
    "              metrics=['mean_squared_logarithmic_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcand\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential_14/dense_46/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential_14/dense_46/embedding_lookup_sparse/Reshape:0\", shape=(None, 100), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential_14/dense_46/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 30ms/step - loss: 4.3355 - mean_squared_logarithmic_error: 18.8038 - val_loss: 4.1600 - val_mean_squared_logarithmic_error: 17.3079\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.15996, saving model to ../Models\\NN_model.h5\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 4.0280 - mean_squared_logarithmic_error: 16.2343 - val_loss: 3.8593 - val_mean_squared_logarithmic_error: 14.8967\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.15996 to 3.85933, saving model to ../Models\\NN_model.h5\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.7724 - mean_squared_logarithmic_error: 14.2408 - val_loss: 3.6157 - val_mean_squared_logarithmic_error: 13.0753\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.85933 to 3.61570, saving model to ../Models\\NN_model.h5\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.5042 - mean_squared_logarithmic_error: 12.2847 - val_loss: 3.4290 - val_mean_squared_logarithmic_error: 11.7598\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.61570 to 3.42900, saving model to ../Models\\NN_model.h5\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.3680 - mean_squared_logarithmic_error: 11.3468 - val_loss: 3.2776 - val_mean_squared_logarithmic_error: 10.7442\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.42900 to 3.27758, saving model to ../Models\\NN_model.h5\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3.2129 - mean_squared_logarithmic_error: 10.3250 - val_loss: 3.1482 - val_mean_squared_logarithmic_error: 9.9127\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.27758 to 3.14820, saving model to ../Models\\NN_model.h5\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.0833 - mean_squared_logarithmic_error: 9.5093 - val_loss: 3.0336 - val_mean_squared_logarithmic_error: 9.2041\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.14820 to 3.03357, saving model to ../Models\\NN_model.h5\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2.9990 - mean_squared_logarithmic_error: 8.9960 - val_loss: 2.9297 - val_mean_squared_logarithmic_error: 8.5845\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.03357 to 2.92968, saving model to ../Models\\NN_model.h5\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 2.8959 - mean_squared_logarithmic_error: 8.3874 - val_loss: 2.8339 - val_mean_squared_logarithmic_error: 8.0323\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.92968 to 2.83388, saving model to ../Models\\NN_model.h5\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2.7752 - mean_squared_logarithmic_error: 7.7034 - val_loss: 2.7446 - val_mean_squared_logarithmic_error: 7.5341\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.83388 to 2.74458, saving model to ../Models\\NN_model.h5\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2.6921 - mean_squared_logarithmic_error: 7.2483 - val_loss: 2.6605 - val_mean_squared_logarithmic_error: 7.0798\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.74458 to 2.66054, saving model to ../Models\\NN_model.h5\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 2.6258 - mean_squared_logarithmic_error: 6.8960 - val_loss: 2.5812 - val_mean_squared_logarithmic_error: 6.6638\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.66054 to 2.58118, saving model to ../Models\\NN_model.h5\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 2.5403 - mean_squared_logarithmic_error: 6.4535 - val_loss: 2.5061 - val_mean_squared_logarithmic_error: 6.2816\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.58118 to 2.50606, saving model to ../Models\\NN_model.h5\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2.4675 - mean_squared_logarithmic_error: 6.0897 - val_loss: 2.4344 - val_mean_squared_logarithmic_error: 5.9275\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.50606 to 2.43440, saving model to ../Models\\NN_model.h5\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2.3935 - mean_squared_logarithmic_error: 5.7295 - val_loss: 2.3664 - val_mean_squared_logarithmic_error: 5.6010\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.43440 to 2.36639, saving model to ../Models\\NN_model.h5\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2.3406 - mean_squared_logarithmic_error: 5.4797 - val_loss: 2.3015 - val_mean_squared_logarithmic_error: 5.2981\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.36639 to 2.30150, saving model to ../Models\\NN_model.h5\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2.2607 - mean_squared_logarithmic_error: 5.1115 - val_loss: 2.2390 - val_mean_squared_logarithmic_error: 5.0144\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.30150 to 2.23903, saving model to ../Models\\NN_model.h5\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2.2080 - mean_squared_logarithmic_error: 4.8755 - val_loss: 2.1789 - val_mean_squared_logarithmic_error: 4.7489\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.23903 to 2.17894, saving model to ../Models\\NN_model.h5\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 2.1405 - mean_squared_logarithmic_error: 4.5819 - val_loss: 2.1213 - val_mean_squared_logarithmic_error: 4.5010\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.17894 to 2.12129, saving model to ../Models\\NN_model.h5\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2.0917 - mean_squared_logarithmic_error: 4.3761 - val_loss: 2.0659 - val_mean_squared_logarithmic_error: 4.2691\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.12129 to 2.06590, saving model to ../Models\\NN_model.h5\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2.0350 - mean_squared_logarithmic_error: 4.1417 - val_loss: 2.0124 - val_mean_squared_logarithmic_error: 4.0508\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.06590 to 2.01237, saving model to ../Models\\NN_model.h5\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.9810 - mean_squared_logarithmic_error: 3.9255 - val_loss: 1.9609 - val_mean_squared_logarithmic_error: 3.8461\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.01237 to 1.96085, saving model to ../Models\\NN_model.h5\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.9445 - mean_squared_logarithmic_error: 3.7819 - val_loss: 1.9115 - val_mean_squared_logarithmic_error: 3.6550\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.96085 to 1.91149, saving model to ../Models\\NN_model.h5\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1.8706 - mean_squared_logarithmic_error: 3.4999 - val_loss: 1.8637 - val_mean_squared_logarithmic_error: 3.4744\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.91149 to 1.86365, saving model to ../Models\\NN_model.h5\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.8343 - mean_squared_logarithmic_error: 3.3653 - val_loss: 1.8180 - val_mean_squared_logarithmic_error: 3.3065\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.86365 to 1.81803, saving model to ../Models\\NN_model.h5\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.8006 - mean_squared_logarithmic_error: 3.2436 - val_loss: 1.7738 - val_mean_squared_logarithmic_error: 3.1477\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.81803 to 1.77381, saving model to ../Models\\NN_model.h5\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.7476 - mean_squared_logarithmic_error: 3.0548 - val_loss: 1.7308 - val_mean_squared_logarithmic_error: 2.9970\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.77381 to 1.73079, saving model to ../Models\\NN_model.h5\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.6969 - mean_squared_logarithmic_error: 2.8803 - val_loss: 1.6890 - val_mean_squared_logarithmic_error: 2.8541\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.73079 to 1.68896, saving model to ../Models\\NN_model.h5\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.6654 - mean_squared_logarithmic_error: 2.7748 - val_loss: 1.6486 - val_mean_squared_logarithmic_error: 2.7194\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.68896 to 1.64860, saving model to ../Models\\NN_model.h5\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.6170 - mean_squared_logarithmic_error: 2.6152 - val_loss: 1.6094 - val_mean_squared_logarithmic_error: 2.5918\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.64860 to 1.60942, saving model to ../Models\\NN_model.h5\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1.5914 - mean_squared_logarithmic_error: 2.5353 - val_loss: 1.5715 - val_mean_squared_logarithmic_error: 2.4714\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.60942 to 1.57153, saving model to ../Models\\NN_model.h5\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.5595 - mean_squared_logarithmic_error: 2.4330 - val_loss: 1.5346 - val_mean_squared_logarithmic_error: 2.3568\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.57153 to 1.53457, saving model to ../Models\\NN_model.h5\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.4958 - mean_squared_logarithmic_error: 2.2378 - val_loss: 1.4985 - val_mean_squared_logarithmic_error: 2.2475\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.53457 to 1.49851, saving model to ../Models\\NN_model.h5\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.4935 - mean_squared_logarithmic_error: 2.2318 - val_loss: 1.4640 - val_mean_squared_logarithmic_error: 2.1454\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.49851 to 1.46400, saving model to ../Models\\NN_model.h5\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1.4293 - mean_squared_logarithmic_error: 2.0439 - val_loss: 1.4301 - val_mean_squared_logarithmic_error: 2.0473\n",
      "\n",
      "Epoch 00035: val_loss improved from 1.46400 to 1.43006, saving model to ../Models\\NN_model.h5\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.4279 - mean_squared_logarithmic_error: 2.0414 - val_loss: 1.3970 - val_mean_squared_logarithmic_error: 1.9541\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.43006 to 1.39702, saving model to ../Models\\NN_model.h5\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1.3861 - mean_squared_logarithmic_error: 1.9242 - val_loss: 1.3657 - val_mean_squared_logarithmic_error: 1.8678\n",
      "\n",
      "Epoch 00037: val_loss improved from 1.39702 to 1.36574, saving model to ../Models\\NN_model.h5\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.3474 - mean_squared_logarithmic_error: 1.8168 - val_loss: 1.3351 - val_mean_squared_logarithmic_error: 1.7853\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.36574 to 1.33510, saving model to ../Models\\NN_model.h5\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1.3144 - mean_squared_logarithmic_error: 1.7285 - val_loss: 1.3051 - val_mean_squared_logarithmic_error: 1.7062\n",
      "\n",
      "Epoch 00039: val_loss improved from 1.33510 to 1.30505, saving model to ../Models\\NN_model.h5\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1.3101 - mean_squared_logarithmic_error: 1.7199 - val_loss: 1.2762 - val_mean_squared_logarithmic_error: 1.6319\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.30505 to 1.27619, saving model to ../Models\\NN_model.h5\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.2307 - mean_squared_logarithmic_error: 1.5156 - val_loss: 1.2477 - val_mean_squared_logarithmic_error: 1.5602\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.27619 to 1.24771, saving model to ../Models\\NN_model.h5\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.2375 - mean_squared_logarithmic_error: 1.5320 - val_loss: 1.2200 - val_mean_squared_logarithmic_error: 1.4921\n",
      "\n",
      "Epoch 00042: val_loss improved from 1.24771 to 1.22001, saving model to ../Models\\NN_model.h5\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.1989 - mean_squared_logarithmic_error: 1.4394 - val_loss: 1.1932 - val_mean_squared_logarithmic_error: 1.4277\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.22001 to 1.19321, saving model to ../Models\\NN_model.h5\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.1901 - mean_squared_logarithmic_error: 1.4207 - val_loss: 1.1670 - val_mean_squared_logarithmic_error: 1.3661\n",
      "\n",
      "Epoch 00044: val_loss improved from 1.19321 to 1.16697, saving model to ../Models\\NN_model.h5\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1.1460 - mean_squared_logarithmic_error: 1.3139 - val_loss: 1.1414 - val_mean_squared_logarithmic_error: 1.3073\n",
      "\n",
      "Epoch 00045: val_loss improved from 1.16697 to 1.14137, saving model to ../Models\\NN_model.h5\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.1066 - mean_squared_logarithmic_error: 1.2262 - val_loss: 1.1163 - val_mean_squared_logarithmic_error: 1.2510\n",
      "\n",
      "Epoch 00046: val_loss improved from 1.14137 to 1.11626, saving model to ../Models\\NN_model.h5\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.0840 - mean_squared_logarithmic_error: 1.1785 - val_loss: 1.0921 - val_mean_squared_logarithmic_error: 1.1981\n",
      "\n",
      "Epoch 00047: val_loss improved from 1.11626 to 1.09214, saving model to ../Models\\NN_model.h5\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.0280 - mean_squared_logarithmic_error: 1.0586 - val_loss: 1.0679 - val_mean_squared_logarithmic_error: 1.1461\n",
      "\n",
      "Epoch 00048: val_loss improved from 1.09214 to 1.06790, saving model to ../Models\\NN_model.h5\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1.0706 - mean_squared_logarithmic_error: 1.1494 - val_loss: 1.0448 - val_mean_squared_logarithmic_error: 1.0976\n",
      "\n",
      "Epoch 00049: val_loss improved from 1.06790 to 1.04478, saving model to ../Models\\NN_model.h5\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.0464 - mean_squared_logarithmic_error: 1.1015 - val_loss: 1.0232 - val_mean_squared_logarithmic_error: 1.0534\n",
      "\n",
      "Epoch 00050: val_loss improved from 1.04478 to 1.02318, saving model to ../Models\\NN_model.h5\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1.0162 - mean_squared_logarithmic_error: 1.0365 - val_loss: 1.0021 - val_mean_squared_logarithmic_error: 1.0111\n",
      "\n",
      "Epoch 00051: val_loss improved from 1.02318 to 1.00210, saving model to ../Models\\NN_model.h5\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.9798 - mean_squared_logarithmic_error: 0.9655 - val_loss: 0.9821 - val_mean_squared_logarithmic_error: 0.9720\n",
      "\n",
      "Epoch 00052: val_loss improved from 1.00210 to 0.98214, saving model to ../Models\\NN_model.h5\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.9980 - mean_squared_logarithmic_error: 0.9974 - val_loss: 0.9626 - val_mean_squared_logarithmic_error: 0.9344\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.98214 to 0.96256, saving model to ../Models\\NN_model.h5\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.9196 - mean_squared_logarithmic_error: 0.8481 - val_loss: 0.9431 - val_mean_squared_logarithmic_error: 0.8978\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.96256 to 0.94315, saving model to ../Models\\NN_model.h5\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.9621 - mean_squared_logarithmic_error: 0.9336 - val_loss: 0.9243 - val_mean_squared_logarithmic_error: 0.8632\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.94315 to 0.92433, saving model to ../Models\\NN_model.h5\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.9407 - mean_squared_logarithmic_error: 0.8918 - val_loss: 0.9068 - val_mean_squared_logarithmic_error: 0.8317\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.92433 to 0.90683, saving model to ../Models\\NN_model.h5\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.8878 - mean_squared_logarithmic_error: 0.7944 - val_loss: 0.8899 - val_mean_squared_logarithmic_error: 0.8018\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.90683 to 0.88992, saving model to ../Models\\NN_model.h5\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.9216 - mean_squared_logarithmic_error: 0.8545 - val_loss: 0.8731 - val_mean_squared_logarithmic_error: 0.7727\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.88992 to 0.87308, saving model to ../Models\\NN_model.h5\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.8873 - mean_squared_logarithmic_error: 0.7938 - val_loss: 0.8572 - val_mean_squared_logarithmic_error: 0.7458\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.87308 to 0.85723, saving model to ../Models\\NN_model.h5\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.8530 - mean_squared_logarithmic_error: 0.7303 - val_loss: 0.8417 - val_mean_squared_logarithmic_error: 0.7200\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.85723 to 0.84169, saving model to ../Models\\NN_model.h5\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.8940 - mean_squared_logarithmic_error: 0.8050 - val_loss: 0.8270 - val_mean_squared_logarithmic_error: 0.6961\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.84169 to 0.82702, saving model to ../Models\\NN_model.h5\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.8860 - mean_squared_logarithmic_error: 0.7961 - val_loss: 0.8139 - val_mean_squared_logarithmic_error: 0.6751\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.82702 to 0.81389, saving model to ../Models\\NN_model.h5\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.7706 - mean_squared_logarithmic_error: 0.5976 - val_loss: 0.7999 - val_mean_squared_logarithmic_error: 0.6532\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.81389 to 0.79989, saving model to ../Models\\NN_model.h5\n",
      "Epoch 64/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.7566 - mean_squared_logarithmic_error: 0.5847 - val_loss: 0.7855 - val_mean_squared_logarithmic_error: 0.6311\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.79989 to 0.78549, saving model to ../Models\\NN_model.h5\n",
      "Epoch 65/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.7766 - mean_squared_logarithmic_error: 0.6113 - val_loss: 0.7713 - val_mean_squared_logarithmic_error: 0.6098\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.78549 to 0.77133, saving model to ../Models\\NN_model.h5\n",
      "Epoch 66/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.8100 - mean_squared_logarithmic_error: 0.6679 - val_loss: 0.7588 - val_mean_squared_logarithmic_error: 0.5914\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.77133 to 0.75882, saving model to ../Models\\NN_model.h5\n",
      "Epoch 67/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.7598 - mean_squared_logarithmic_error: 0.5903 - val_loss: 0.7468 - val_mean_squared_logarithmic_error: 0.5740\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.75882 to 0.74678, saving model to ../Models\\NN_model.h5\n",
      "Epoch 68/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.7217 - mean_squared_logarithmic_error: 0.5228 - val_loss: 0.7346 - val_mean_squared_logarithmic_error: 0.5568\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.74678 to 0.73465, saving model to ../Models\\NN_model.h5\n",
      "Epoch 69/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.7347 - mean_squared_logarithmic_error: 0.5544 - val_loss: 0.7226 - val_mean_squared_logarithmic_error: 0.5401\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.73465 to 0.72261, saving model to ../Models\\NN_model.h5\n",
      "Epoch 70/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.7106 - mean_squared_logarithmic_error: 0.5113 - val_loss: 0.7112 - val_mean_squared_logarithmic_error: 0.5246\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.72261 to 0.71125, saving model to ../Models\\NN_model.h5\n",
      "Epoch 71/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.7604 - mean_squared_logarithmic_error: 0.5888 - val_loss: 0.7009 - val_mean_squared_logarithmic_error: 0.5108\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.71125 to 0.70095, saving model to ../Models\\NN_model.h5\n",
      "Epoch 72/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.7129 - mean_squared_logarithmic_error: 0.5332 - val_loss: 0.6908 - val_mean_squared_logarithmic_error: 0.4976\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.70095 to 0.69084, saving model to ../Models\\NN_model.h5\n",
      "Epoch 73/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.6920 - mean_squared_logarithmic_error: 0.4884 - val_loss: 0.6813 - val_mean_squared_logarithmic_error: 0.4853\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.69084 to 0.68134, saving model to ../Models\\NN_model.h5\n",
      "Epoch 74/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.6888 - mean_squared_logarithmic_error: 0.4948 - val_loss: 0.6716 - val_mean_squared_logarithmic_error: 0.4730\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.68134 to 0.67164, saving model to ../Models\\NN_model.h5\n",
      "Epoch 75/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.7250 - mean_squared_logarithmic_error: 0.5418 - val_loss: 0.6633 - val_mean_squared_logarithmic_error: 0.4626\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.67164 to 0.66329, saving model to ../Models\\NN_model.h5\n",
      "Epoch 76/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.7107 - mean_squared_logarithmic_error: 0.5317 - val_loss: 0.6552 - val_mean_squared_logarithmic_error: 0.4527\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.66329 to 0.65517, saving model to ../Models\\NN_model.h5\n",
      "Epoch 77/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.6267 - mean_squared_logarithmic_error: 0.4151 - val_loss: 0.6472 - val_mean_squared_logarithmic_error: 0.4431\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.65517 to 0.64719, saving model to ../Models\\NN_model.h5\n",
      "Epoch 78/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.7081 - mean_squared_logarithmic_error: 0.5209 - val_loss: 0.6393 - val_mean_squared_logarithmic_error: 0.4337\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.64719 to 0.63928, saving model to ../Models\\NN_model.h5\n",
      "Epoch 79/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.6778 - mean_squared_logarithmic_error: 0.4733 - val_loss: 0.6319 - val_mean_squared_logarithmic_error: 0.4251\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.63928 to 0.63188, saving model to ../Models\\NN_model.h5\n",
      "Epoch 80/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.6634 - mean_squared_logarithmic_error: 0.4553 - val_loss: 0.6249 - val_mean_squared_logarithmic_error: 0.4172\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.63188 to 0.62491, saving model to ../Models\\NN_model.h5\n",
      "Epoch 81/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.6911 - mean_squared_logarithmic_error: 0.5040 - val_loss: 0.6182 - val_mean_squared_logarithmic_error: 0.4096\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.62491 to 0.61815, saving model to ../Models\\NN_model.h5\n",
      "Epoch 82/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.6669 - mean_squared_logarithmic_error: 0.4634 - val_loss: 0.6122 - val_mean_squared_logarithmic_error: 0.4030\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.61815 to 0.61223, saving model to ../Models\\NN_model.h5\n",
      "Epoch 83/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.6059 - mean_squared_logarithmic_error: 0.3886 - val_loss: 0.6059 - val_mean_squared_logarithmic_error: 0.3962\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.61223 to 0.60594, saving model to ../Models\\NN_model.h5\n",
      "Epoch 84/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5880 - mean_squared_logarithmic_error: 0.3896 - val_loss: 0.5994 - val_mean_squared_logarithmic_error: 0.3892\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.60594 to 0.59938, saving model to ../Models\\NN_model.h5\n",
      "Epoch 85/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5749 - mean_squared_logarithmic_error: 0.3503 - val_loss: 0.5933 - val_mean_squared_logarithmic_error: 0.3829\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.59938 to 0.59333, saving model to ../Models\\NN_model.h5\n",
      "Epoch 86/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5441 - mean_squared_logarithmic_error: 0.3184 - val_loss: 0.5874 - val_mean_squared_logarithmic_error: 0.3768\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.59333 to 0.58741, saving model to ../Models\\NN_model.h5\n",
      "Epoch 87/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.6201 - mean_squared_logarithmic_error: 0.3904 - val_loss: 0.5823 - val_mean_squared_logarithmic_error: 0.3717\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.58741 to 0.58233, saving model to ../Models\\NN_model.h5\n",
      "Epoch 88/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.6679 - mean_squared_logarithmic_error: 0.4545 - val_loss: 0.5780 - val_mean_squared_logarithmic_error: 0.3673\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.58233 to 0.57800, saving model to ../Models\\NN_model.h5\n",
      "Epoch 89/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5645 - mean_squared_logarithmic_error: 0.3292 - val_loss: 0.5738 - val_mean_squared_logarithmic_error: 0.3631\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.57800 to 0.57377, saving model to ../Models\\NN_model.h5\n",
      "Epoch 90/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5939 - mean_squared_logarithmic_error: 0.3807 - val_loss: 0.5695 - val_mean_squared_logarithmic_error: 0.3590\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.57377 to 0.56954, saving model to ../Models\\NN_model.h5\n",
      "Epoch 91/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5378 - mean_squared_logarithmic_error: 0.2938 - val_loss: 0.5655 - val_mean_squared_logarithmic_error: 0.3550\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.56954 to 0.56547, saving model to ../Models\\NN_model.h5\n",
      "Epoch 92/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5610 - mean_squared_logarithmic_error: 0.3246 - val_loss: 0.5616 - val_mean_squared_logarithmic_error: 0.3513\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.56547 to 0.56156, saving model to ../Models\\NN_model.h5\n",
      "Epoch 93/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5565 - mean_squared_logarithmic_error: 0.3380 - val_loss: 0.5578 - val_mean_squared_logarithmic_error: 0.3478\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.56156 to 0.55778, saving model to ../Models\\NN_model.h5\n",
      "Epoch 94/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5950 - mean_squared_logarithmic_error: 0.3775 - val_loss: 0.5542 - val_mean_squared_logarithmic_error: 0.3446\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.55778 to 0.55423, saving model to ../Models\\NN_model.h5\n",
      "Epoch 95/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.5404 - mean_squared_logarithmic_error: 0.3136 - val_loss: 0.5508 - val_mean_squared_logarithmic_error: 0.3415\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.55423 to 0.55082, saving model to ../Models\\NN_model.h5\n",
      "Epoch 96/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5912 - mean_squared_logarithmic_error: 0.3709 - val_loss: 0.5479 - val_mean_squared_logarithmic_error: 0.3390\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.55082 to 0.54791, saving model to ../Models\\NN_model.h5\n",
      "Epoch 97/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5653 - mean_squared_logarithmic_error: 0.3365 - val_loss: 0.5451 - val_mean_squared_logarithmic_error: 0.3365\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.54791 to 0.54512, saving model to ../Models\\NN_model.h5\n",
      "Epoch 98/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5528 - mean_squared_logarithmic_error: 0.3567 - val_loss: 0.5423 - val_mean_squared_logarithmic_error: 0.3340\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.54512 to 0.54231, saving model to ../Models\\NN_model.h5\n",
      "Epoch 99/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4868 - mean_squared_logarithmic_error: 0.2622 - val_loss: 0.5393 - val_mean_squared_logarithmic_error: 0.3315\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.54231 to 0.53931, saving model to ../Models\\NN_model.h5\n",
      "Epoch 100/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4966 - mean_squared_logarithmic_error: 0.2711 - val_loss: 0.5365 - val_mean_squared_logarithmic_error: 0.3292\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.53931 to 0.53655, saving model to ../Models\\NN_model.h5\n",
      "Epoch 101/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5956 - mean_squared_logarithmic_error: 0.4076 - val_loss: 0.5342 - val_mean_squared_logarithmic_error: 0.3272\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.53655 to 0.53416, saving model to ../Models\\NN_model.h5\n",
      "Epoch 102/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5215 - mean_squared_logarithmic_error: 0.3206 - val_loss: 0.5318 - val_mean_squared_logarithmic_error: 0.3253\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.53416 to 0.53183, saving model to ../Models\\NN_model.h5\n",
      "Epoch 103/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5831 - mean_squared_logarithmic_error: 0.3616 - val_loss: 0.5299 - val_mean_squared_logarithmic_error: 0.3238\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.53183 to 0.52986, saving model to ../Models\\NN_model.h5\n",
      "Epoch 104/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.6190 - mean_squared_logarithmic_error: 0.4411 - val_loss: 0.5282 - val_mean_squared_logarithmic_error: 0.3224\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.52986 to 0.52817, saving model to ../Models\\NN_model.h5\n",
      "Epoch 105/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.6185 - mean_squared_logarithmic_error: 0.3949 - val_loss: 0.5268 - val_mean_squared_logarithmic_error: 0.3213\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.52817 to 0.52679, saving model to ../Models\\NN_model.h5\n",
      "Epoch 106/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5353 - mean_squared_logarithmic_error: 0.3008 - val_loss: 0.5255 - val_mean_squared_logarithmic_error: 0.3203\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.52679 to 0.52552, saving model to ../Models\\NN_model.h5\n",
      "Epoch 107/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5706 - mean_squared_logarithmic_error: 0.3673 - val_loss: 0.5243 - val_mean_squared_logarithmic_error: 0.3192\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.52552 to 0.52431, saving model to ../Models\\NN_model.h5\n",
      "Epoch 108/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5350 - mean_squared_logarithmic_error: 0.3070 - val_loss: 0.5229 - val_mean_squared_logarithmic_error: 0.3181\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.52431 to 0.52289, saving model to ../Models\\NN_model.h5\n",
      "Epoch 109/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5534 - mean_squared_logarithmic_error: 0.3460 - val_loss: 0.5216 - val_mean_squared_logarithmic_error: 0.3171\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.52289 to 0.52162, saving model to ../Models\\NN_model.h5\n",
      "Epoch 110/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.6231 - mean_squared_logarithmic_error: 0.4229 - val_loss: 0.5205 - val_mean_squared_logarithmic_error: 0.3163\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.52162 to 0.52052, saving model to ../Models\\NN_model.h5\n",
      "Epoch 111/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5852 - mean_squared_logarithmic_error: 0.3796 - val_loss: 0.5195 - val_mean_squared_logarithmic_error: 0.3155\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.52052 to 0.51951, saving model to ../Models\\NN_model.h5\n",
      "Epoch 112/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4970 - mean_squared_logarithmic_error: 0.2857 - val_loss: 0.5184 - val_mean_squared_logarithmic_error: 0.3146\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.51951 to 0.51840, saving model to ../Models\\NN_model.h5\n",
      "Epoch 113/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5612 - mean_squared_logarithmic_error: 0.3476 - val_loss: 0.5172 - val_mean_squared_logarithmic_error: 0.3138\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.51840 to 0.51724, saving model to ../Models\\NN_model.h5\n",
      "Epoch 114/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4780 - mean_squared_logarithmic_error: 0.2529 - val_loss: 0.5161 - val_mean_squared_logarithmic_error: 0.3130\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.51724 to 0.51614, saving model to ../Models\\NN_model.h5\n",
      "Epoch 115/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5920 - mean_squared_logarithmic_error: 0.3743 - val_loss: 0.5152 - val_mean_squared_logarithmic_error: 0.3123\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.51614 to 0.51524, saving model to ../Models\\NN_model.h5\n",
      "Epoch 116/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4744 - mean_squared_logarithmic_error: 0.2459 - val_loss: 0.5145 - val_mean_squared_logarithmic_error: 0.3118\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.51524 to 0.51449, saving model to ../Models\\NN_model.h5\n",
      "Epoch 117/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.6287 - mean_squared_logarithmic_error: 0.4080 - val_loss: 0.5140 - val_mean_squared_logarithmic_error: 0.3113\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.51449 to 0.51395, saving model to ../Models\\NN_model.h5\n",
      "Epoch 118/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.6083 - mean_squared_logarithmic_error: 0.4114 - val_loss: 0.5135 - val_mean_squared_logarithmic_error: 0.3109\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.51395 to 0.51349, saving model to ../Models\\NN_model.h5\n",
      "Epoch 119/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5971 - mean_squared_logarithmic_error: 0.4057 - val_loss: 0.5130 - val_mean_squared_logarithmic_error: 0.3105\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.51349 to 0.51302, saving model to ../Models\\NN_model.h5\n",
      "Epoch 120/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5175 - mean_squared_logarithmic_error: 0.3306 - val_loss: 0.5126 - val_mean_squared_logarithmic_error: 0.3101\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.51302 to 0.51257, saving model to ../Models\\NN_model.h5\n",
      "Epoch 121/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.4715 - mean_squared_logarithmic_error: 0.2373 - val_loss: 0.5119 - val_mean_squared_logarithmic_error: 0.3097\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.51257 to 0.51191, saving model to ../Models\\NN_model.h5\n",
      "Epoch 122/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5427 - mean_squared_logarithmic_error: 0.3376 - val_loss: 0.5114 - val_mean_squared_logarithmic_error: 0.3094\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.51191 to 0.51140, saving model to ../Models\\NN_model.h5\n",
      "Epoch 123/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5531 - mean_squared_logarithmic_error: 0.3323 - val_loss: 0.5108 - val_mean_squared_logarithmic_error: 0.3091\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.51140 to 0.51080, saving model to ../Models\\NN_model.h5\n",
      "Epoch 124/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4413 - mean_squared_logarithmic_error: 0.2148 - val_loss: 0.5102 - val_mean_squared_logarithmic_error: 0.3087\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.51080 to 0.51018, saving model to ../Models\\NN_model.h5\n",
      "Epoch 125/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3800 - mean_squared_logarithmic_error: 0.1566 - val_loss: 0.5095 - val_mean_squared_logarithmic_error: 0.3084\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.51018 to 0.50949, saving model to ../Models\\NN_model.h5\n",
      "Epoch 126/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5907 - mean_squared_logarithmic_error: 0.3738 - val_loss: 0.5089 - val_mean_squared_logarithmic_error: 0.3081\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.50949 to 0.50887, saving model to ../Models\\NN_model.h5\n",
      "Epoch 127/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.6122 - mean_squared_logarithmic_error: 0.4162 - val_loss: 0.5085 - val_mean_squared_logarithmic_error: 0.3078\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.50887 to 0.50854, saving model to ../Models\\NN_model.h5\n",
      "Epoch 128/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5534 - mean_squared_logarithmic_error: 0.3494 - val_loss: 0.5082 - val_mean_squared_logarithmic_error: 0.3076\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.50854 to 0.50816, saving model to ../Models\\NN_model.h5\n",
      "Epoch 129/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.4802 - mean_squared_logarithmic_error: 0.2729 - val_loss: 0.5078 - val_mean_squared_logarithmic_error: 0.3074\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.50816 to 0.50782, saving model to ../Models\\NN_model.h5\n",
      "Epoch 130/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3869 - mean_squared_logarithmic_error: 0.1724 - val_loss: 0.5074 - val_mean_squared_logarithmic_error: 0.3073\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.50782 to 0.50737, saving model to ../Models\\NN_model.h5\n",
      "Epoch 131/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.6607 - mean_squared_logarithmic_error: 0.4781 - val_loss: 0.5071 - val_mean_squared_logarithmic_error: 0.3070\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.50737 to 0.50708, saving model to ../Models\\NN_model.h5\n",
      "Epoch 132/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4224 - mean_squared_logarithmic_error: 0.1982 - val_loss: 0.5068 - val_mean_squared_logarithmic_error: 0.3068\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.50708 to 0.50681, saving model to ../Models\\NN_model.h5\n",
      "Epoch 133/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5008 - mean_squared_logarithmic_error: 0.2659 - val_loss: 0.5065 - val_mean_squared_logarithmic_error: 0.3065\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.50681 to 0.50654, saving model to ../Models\\NN_model.h5\n",
      "Epoch 134/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4979 - mean_squared_logarithmic_error: 0.2976 - val_loss: 0.5063 - val_mean_squared_logarithmic_error: 0.3064\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.50654 to 0.50631, saving model to ../Models\\NN_model.h5\n",
      "Epoch 135/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.4891 - mean_squared_logarithmic_error: 0.2844 - val_loss: 0.5061 - val_mean_squared_logarithmic_error: 0.3063\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.50631 to 0.50610, saving model to ../Models\\NN_model.h5\n",
      "Epoch 136/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5679 - mean_squared_logarithmic_error: 0.3668 - val_loss: 0.5059 - val_mean_squared_logarithmic_error: 0.3062\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.50610 to 0.50590, saving model to ../Models\\NN_model.h5\n",
      "Epoch 137/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5419 - mean_squared_logarithmic_error: 0.3194 - val_loss: 0.5057 - val_mean_squared_logarithmic_error: 0.3061\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.50590 to 0.50570, saving model to ../Models\\NN_model.h5\n",
      "Epoch 138/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5550 - mean_squared_logarithmic_error: 0.3354 - val_loss: 0.5056 - val_mean_squared_logarithmic_error: 0.3059\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.50570 to 0.50556, saving model to ../Models\\NN_model.h5\n",
      "Epoch 139/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5333 - mean_squared_logarithmic_error: 0.3229 - val_loss: 0.5054 - val_mean_squared_logarithmic_error: 0.3058\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.50556 to 0.50542, saving model to ../Models\\NN_model.h5\n",
      "Epoch 140/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4724 - mean_squared_logarithmic_error: 0.2680 - val_loss: 0.5052 - val_mean_squared_logarithmic_error: 0.3056\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.50542 to 0.50520, saving model to ../Models\\NN_model.h5\n",
      "Epoch 141/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.6319 - mean_squared_logarithmic_error: 0.4231 - val_loss: 0.5051 - val_mean_squared_logarithmic_error: 0.3055\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.50520 to 0.50508, saving model to ../Models\\NN_model.h5\n",
      "Epoch 142/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4467 - mean_squared_logarithmic_error: 0.2308 - val_loss: 0.5050 - val_mean_squared_logarithmic_error: 0.3054\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.50508 to 0.50496, saving model to ../Models\\NN_model.h5\n",
      "Epoch 143/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4996 - mean_squared_logarithmic_error: 0.2909 - val_loss: 0.5048 - val_mean_squared_logarithmic_error: 0.3054\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.50496 to 0.50477, saving model to ../Models\\NN_model.h5\n",
      "Epoch 144/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5278 - mean_squared_logarithmic_error: 0.3113 - val_loss: 0.5046 - val_mean_squared_logarithmic_error: 0.3053\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.50477 to 0.50464, saving model to ../Models\\NN_model.h5\n",
      "Epoch 145/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5313 - mean_squared_logarithmic_error: 0.2973 - val_loss: 0.5045 - val_mean_squared_logarithmic_error: 0.3053\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.50464 to 0.50450, saving model to ../Models\\NN_model.h5\n",
      "Epoch 146/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5596 - mean_squared_logarithmic_error: 0.3353 - val_loss: 0.5044 - val_mean_squared_logarithmic_error: 0.3053\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.50450 to 0.50443, saving model to ../Models\\NN_model.h5\n",
      "Epoch 147/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.6305 - mean_squared_logarithmic_error: 0.4316 - val_loss: 0.5044 - val_mean_squared_logarithmic_error: 0.3052\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.50443 to 0.50439, saving model to ../Models\\NN_model.h5\n",
      "Epoch 148/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4610 - mean_squared_logarithmic_error: 0.2566 - val_loss: 0.5042 - val_mean_squared_logarithmic_error: 0.3051\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.50439 to 0.50425, saving model to ../Models\\NN_model.h5\n",
      "Epoch 149/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5372 - mean_squared_logarithmic_error: 0.3338 - val_loss: 0.5041 - val_mean_squared_logarithmic_error: 0.3049\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.50425 to 0.50415, saving model to ../Models\\NN_model.h5\n",
      "Epoch 150/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5783 - mean_squared_logarithmic_error: 0.3718 - val_loss: 0.5041 - val_mean_squared_logarithmic_error: 0.3048\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.50415 to 0.50410, saving model to ../Models\\NN_model.h5\n",
      "Epoch 151/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5525 - mean_squared_logarithmic_error: 0.3329 - val_loss: 0.5041 - val_mean_squared_logarithmic_error: 0.3046\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.50410 to 0.50410, saving model to ../Models\\NN_model.h5\n",
      "Epoch 152/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.6276 - mean_squared_logarithmic_error: 0.4300 - val_loss: 0.5041 - val_mean_squared_logarithmic_error: 0.3045\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.50410 to 0.50407, saving model to ../Models\\NN_model.h5\n",
      "Epoch 153/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5112 - mean_squared_logarithmic_error: 0.2926 - val_loss: 0.5041 - val_mean_squared_logarithmic_error: 0.3043\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.50407 to 0.50407, saving model to ../Models\\NN_model.h5\n",
      "Epoch 154/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4563 - mean_squared_logarithmic_error: 0.2881 - val_loss: 0.5039 - val_mean_squared_logarithmic_error: 0.3043\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.50407 to 0.50394, saving model to ../Models\\NN_model.h5\n",
      "Epoch 155/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5208 - mean_squared_logarithmic_error: 0.2806 - val_loss: 0.5038 - val_mean_squared_logarithmic_error: 0.3043\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.50394 to 0.50380, saving model to ../Models\\NN_model.h5\n",
      "Epoch 156/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5588 - mean_squared_logarithmic_error: 0.3348 - val_loss: 0.5037 - val_mean_squared_logarithmic_error: 0.3043\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.50380 to 0.50374, saving model to ../Models\\NN_model.h5\n",
      "Epoch 157/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.6449 - mean_squared_logarithmic_error: 0.4767 - val_loss: 0.5037 - val_mean_squared_logarithmic_error: 0.3041\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.50374\n",
      "Epoch 158/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5644 - mean_squared_logarithmic_error: 0.3464 - val_loss: 0.5038 - val_mean_squared_logarithmic_error: 0.3041\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.50374\n",
      "Epoch 159/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5707 - mean_squared_logarithmic_error: 0.3612 - val_loss: 0.5038 - val_mean_squared_logarithmic_error: 0.3040\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.50374\n",
      "Epoch 160/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5532 - mean_squared_logarithmic_error: 0.3661 - val_loss: 0.5037 - val_mean_squared_logarithmic_error: 0.3039\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.50374 to 0.50373, saving model to ../Models\\NN_model.h5\n",
      "Epoch 161/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4776 - mean_squared_logarithmic_error: 0.2680 - val_loss: 0.5036 - val_mean_squared_logarithmic_error: 0.3039\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.50373 to 0.50362, saving model to ../Models\\NN_model.h5\n",
      "Epoch 162/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5780 - mean_squared_logarithmic_error: 0.3695 - val_loss: 0.5035 - val_mean_squared_logarithmic_error: 0.3038\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.50362 to 0.50353, saving model to ../Models\\NN_model.h5\n",
      "Epoch 163/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5162 - mean_squared_logarithmic_error: 0.3226 - val_loss: 0.5035 - val_mean_squared_logarithmic_error: 0.3038\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.50353 to 0.50348, saving model to ../Models\\NN_model.h5\n",
      "Epoch 164/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5234 - mean_squared_logarithmic_error: 0.3035 - val_loss: 0.5034 - val_mean_squared_logarithmic_error: 0.3038\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.50348 to 0.50336, saving model to ../Models\\NN_model.h5\n",
      "Epoch 165/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4650 - mean_squared_logarithmic_error: 0.2515 - val_loss: 0.5033 - val_mean_squared_logarithmic_error: 0.3037\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.50336 to 0.50329, saving model to ../Models\\NN_model.h5\n",
      "Epoch 166/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4478 - mean_squared_logarithmic_error: 0.2233 - val_loss: 0.5031 - val_mean_squared_logarithmic_error: 0.3037\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.50329 to 0.50315, saving model to ../Models\\NN_model.h5\n",
      "Epoch 167/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5062 - mean_squared_logarithmic_error: 0.2907 - val_loss: 0.5030 - val_mean_squared_logarithmic_error: 0.3037\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.50315 to 0.50303, saving model to ../Models\\NN_model.h5\n",
      "Epoch 168/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4651 - mean_squared_logarithmic_error: 0.2229 - val_loss: 0.5030 - val_mean_squared_logarithmic_error: 0.3038\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.50303 to 0.50298, saving model to ../Models\\NN_model.h5\n",
      "Epoch 169/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.4274 - mean_squared_logarithmic_error: 0.2017 - val_loss: 0.5029 - val_mean_squared_logarithmic_error: 0.3039\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.50298 to 0.50287, saving model to ../Models\\NN_model.h5\n",
      "Epoch 170/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5575 - mean_squared_logarithmic_error: 0.3358 - val_loss: 0.5028 - val_mean_squared_logarithmic_error: 0.3039\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.50287 to 0.50277, saving model to ../Models\\NN_model.h5\n",
      "Epoch 171/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5376 - mean_squared_logarithmic_error: 0.3044 - val_loss: 0.5027 - val_mean_squared_logarithmic_error: 0.3037\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.50277 to 0.50271, saving model to ../Models\\NN_model.h5\n",
      "Epoch 172/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5603 - mean_squared_logarithmic_error: 0.3382 - val_loss: 0.5027 - val_mean_squared_logarithmic_error: 0.3035\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.50271 to 0.50268, saving model to ../Models\\NN_model.h5\n",
      "Epoch 173/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.5342 - mean_squared_logarithmic_error: 0.3095 - val_loss: 0.5027 - val_mean_squared_logarithmic_error: 0.3033\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.50268\n",
      "Epoch 174/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5397 - mean_squared_logarithmic_error: 0.3570 - val_loss: 0.5027 - val_mean_squared_logarithmic_error: 0.3032\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.50268 to 0.50268, saving model to ../Models\\NN_model.h5\n",
      "Epoch 175/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.6357 - mean_squared_logarithmic_error: 0.4241 - val_loss: 0.5027 - val_mean_squared_logarithmic_error: 0.3031\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.50268\n",
      "Epoch 176/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5439 - mean_squared_logarithmic_error: 0.3297 - val_loss: 0.5027 - val_mean_squared_logarithmic_error: 0.3029\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.50268\n",
      "Epoch 177/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3277 - mean_squared_logarithmic_error: 0.1220 - val_loss: 0.5026 - val_mean_squared_logarithmic_error: 0.3028\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.50268 to 0.50263, saving model to ../Models\\NN_model.h5\n",
      "Epoch 178/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.4186 - mean_squared_logarithmic_error: 0.2150 - val_loss: 0.5024 - val_mean_squared_logarithmic_error: 0.3030\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.50263 to 0.50243, saving model to ../Models\\NN_model.h5\n",
      "Epoch 179/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4851 - mean_squared_logarithmic_error: 0.2698 - val_loss: 0.5023 - val_mean_squared_logarithmic_error: 0.3031\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.50243 to 0.50230, saving model to ../Models\\NN_model.h5\n",
      "Epoch 180/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.6160 - mean_squared_logarithmic_error: 0.4391 - val_loss: 0.5022 - val_mean_squared_logarithmic_error: 0.3030\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.50230 to 0.50222, saving model to ../Models\\NN_model.h5\n",
      "Epoch 181/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4182 - mean_squared_logarithmic_error: 0.2133 - val_loss: 0.5022 - val_mean_squared_logarithmic_error: 0.3030\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.50222 to 0.50216, saving model to ../Models\\NN_model.h5\n",
      "Epoch 182/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5736 - mean_squared_logarithmic_error: 0.3629 - val_loss: 0.5021 - val_mean_squared_logarithmic_error: 0.3031\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.50216 to 0.50211, saving model to ../Models\\NN_model.h5\n",
      "Epoch 183/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.6548 - mean_squared_logarithmic_error: 0.4555 - val_loss: 0.5021 - val_mean_squared_logarithmic_error: 0.3029\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.50211 to 0.50209, saving model to ../Models\\NN_model.h5\n",
      "Epoch 184/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4780 - mean_squared_logarithmic_error: 0.2406 - val_loss: 0.5021 - val_mean_squared_logarithmic_error: 0.3028\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.50209 to 0.50208, saving model to ../Models\\NN_model.h5\n",
      "Epoch 185/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5055 - mean_squared_logarithmic_error: 0.2843 - val_loss: 0.5021 - val_mean_squared_logarithmic_error: 0.3027\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.50208 to 0.50205, saving model to ../Models\\NN_model.h5\n",
      "Epoch 186/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5267 - mean_squared_logarithmic_error: 0.3193 - val_loss: 0.5020 - val_mean_squared_logarithmic_error: 0.3026\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.50205 to 0.50202, saving model to ../Models\\NN_model.h5\n",
      "Epoch 187/300\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.4714 - mean_squared_logarithmic_error: 0.22 - 0s 10ms/step - loss: 0.5162 - mean_squared_logarithmic_error: 0.3342 - val_loss: 0.5020 - val_mean_squared_logarithmic_error: 0.3025\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.50202 to 0.50198, saving model to ../Models\\NN_model.h5\n",
      "Epoch 188/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4521 - mean_squared_logarithmic_error: 0.2368 - val_loss: 0.5019 - val_mean_squared_logarithmic_error: 0.3026\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.50198 to 0.50187, saving model to ../Models\\NN_model.h5\n",
      "Epoch 189/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4688 - mean_squared_logarithmic_error: 0.2602 - val_loss: 0.5018 - val_mean_squared_logarithmic_error: 0.3027\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.50187 to 0.50182, saving model to ../Models\\NN_model.h5\n",
      "Epoch 190/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3842 - mean_squared_logarithmic_error: 0.1692 - val_loss: 0.5018 - val_mean_squared_logarithmic_error: 0.3028\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.50182 to 0.50177, saving model to ../Models\\NN_model.h5\n",
      "Epoch 191/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.5622 - mean_squared_logarithmic_error: 0.3410 - val_loss: 0.5017 - val_mean_squared_logarithmic_error: 0.3028\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.50177 to 0.50170, saving model to ../Models\\NN_model.h5\n",
      "Epoch 192/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4306 - mean_squared_logarithmic_error: 0.2202 - val_loss: 0.5016 - val_mean_squared_logarithmic_error: 0.3028\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.50170 to 0.50161, saving model to ../Models\\NN_model.h5\n",
      "Epoch 193/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5211 - mean_squared_logarithmic_error: 0.2983 - val_loss: 0.5016 - val_mean_squared_logarithmic_error: 0.3028\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.50161 to 0.50155, saving model to ../Models\\NN_model.h5\n",
      "Epoch 194/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5430 - mean_squared_logarithmic_error: 0.3134 - val_loss: 0.5015 - val_mean_squared_logarithmic_error: 0.3028\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.50155 to 0.50152, saving model to ../Models\\NN_model.h5\n",
      "Epoch 195/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5806 - mean_squared_logarithmic_error: 0.3672 - val_loss: 0.5015 - val_mean_squared_logarithmic_error: 0.3026\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.50152\n",
      "Epoch 196/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5282 - mean_squared_logarithmic_error: 0.3167 - val_loss: 0.5016 - val_mean_squared_logarithmic_error: 0.3025\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.50152\n",
      "Epoch 197/300\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.6361 - mean_squared_logarithmic_error: 0.4276 - val_loss: 0.5015 - val_mean_squared_logarithmic_error: 0.3024\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.50152\n",
      "Epoch 198/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5383 - mean_squared_logarithmic_error: 0.3089 - val_loss: 0.5015 - val_mean_squared_logarithmic_error: 0.3023\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.50152\n",
      "Epoch 199/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5271 - mean_squared_logarithmic_error: 0.3064 - val_loss: 0.5016 - val_mean_squared_logarithmic_error: 0.3021\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.50152\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=100,\n",
    "    epochs=300,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[mc, early_stopping],\n",
    "    shuffle=True,\n",
    "    steps_per_epoch=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
